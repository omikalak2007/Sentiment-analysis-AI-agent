{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ddba66b",
   "metadata": {},
   "source": [
    "Importing all the neccessary libraries to build the sentiment analysis AI agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99efe71e",
   "metadata": {},
   "source": [
    "The model will be built using langchain and langgraph because it's the most used library to build AI agents worldwideeeee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77fdafca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, Sequence,List\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from operator import add as add_messages\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.tools import tool\n",
    "from operator import add as add_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9267e3",
   "metadata": {},
   "source": [
    "Importing two llama3 llms with a temperature =0 to avoid hallacunation using Ollama\n",
    "Keep in mind the llm1 and llm2 can we swapped with any llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da72c426",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm1 = OllamaLLM(model=\"llama3\",temperature=0)\n",
    "sentiment_prompt = \"\"\"You are an AI that performs sentiment analysis on a given text.\n",
    "Please output only the sentiment: either \"positive\" or \"negative\", without any other text or explanation.\"\"\"\n",
    "\n",
    "llm2 = OllamaLLM(model=\"llama3\",temperature=0)\n",
    "accuracy_prompt = \"\"\"You are an AI that assesses the accuracy of a sentiment analysis.\n",
    "You will be given a list of original texts and their sentiment analysis results.\n",
    "Analyze if each sentiment is correct. Finally, calculate the overall accuracy as a percentage\n",
    "and output only the percentage number (e.g., 75%), also keep in mind that the sentiment generated by the sentiment analysis\n",
    "is postive or negative it doesn't have neutral\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bd1133",
   "metadata": {},
   "source": [
    "The state is used to store context of llm inputs and outputs in langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2072c669",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    input_texts: List[str]\n",
    "    current_index: int\n",
    "    sentiment_results: List[str]\n",
    "    accuracy_score: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d7b5e4",
   "metadata": {},
   "source": [
    "This function takes the sentiment from the state and invoke it to the llm1 and the llm1 generates whether the sentiment is positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99460bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"This node performs sentiment analysis using LLM1.\"\"\"\n",
    "    \n",
    "    current_index = state['current_index']\n",
    "    texts_to_analyze = state['input_texts']\n",
    "    \n",
    "    # Check if there are texts left to analyze\n",
    "    if current_index >= len(texts_to_analyze):\n",
    "        print(\"All texts have been analyzed. Transitioning to accuracy analysis.\")\n",
    "        return state # The graph's router will handle the transition\n",
    "    \n",
    "    current_text = texts_to_analyze[current_index]\n",
    "    \n",
    "    print(f\"Analyzing text {current_index + 1}: '{current_text}'\")\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=sentiment_prompt),\n",
    "        HumanMessage(content=current_text)\n",
    "    ]\n",
    "    \n",
    "    # Invoke LLM1 and get the sentiment\n",
    "    sentiment_result = llm1.invoke(messages).strip()\n",
    "    \n",
    "    # Append the result to the state\n",
    "    state['sentiment_results'].append(sentiment_result)\n",
    "    state['current_index'] += 1\n",
    "    \n",
    "    print(f\"  -> Result: {sentiment_result}\")\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf08548c",
   "metadata": {},
   "source": [
    "The llm2 in this function critiques the sentiments generated from llm1 and generates the accuracy and the correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "743da598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_analysis_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"This node calculates the accuracy score using LLM2.\"\"\"\n",
    "    \n",
    "    print(\"\\n--- Starting Accuracy Analysis ---\")\n",
    "    \n",
    "    # Prepare the prompt for LLM2 with all data\n",
    "    analysis_data = \"Original Texts and Sentiments:\\n\"\n",
    "    for i, text in enumerate(state['input_texts']):\n",
    "        analysis_data += f\"{i+1}. Text: '{text}' | Sentiment: {state['sentiment_results'][i]}\\n\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=accuracy_prompt),\n",
    "        HumanMessage(content=analysis_data)\n",
    "    ]\n",
    "    \n",
    "    # Invoke LLM2 and get the final accuracy score\n",
    "    accuracy_score = llm2.invoke(messages).strip()\n",
    "    \n",
    "    state['accuracy_score'] = accuracy_score\n",
    "    \n",
    "    print(f\"Final Accuracy Score: {accuracy_score}\")\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dc1b38",
   "metadata": {},
   "source": [
    "This function takes the input list of sentiments and decided whether to stop the process or continue invoking the llm1 and llm2.\n",
    "For example if there are 4 inputs in the list the agent will not stop working until all 4 sentiments are generated and their accuracy.\n",
    "This function works as a router which routes to different actions based on conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a89af02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state: AgentState) -> str:\n",
    "    \"\"\"A router to decide whether to loop or move to accuracy analysis.\"\"\"\n",
    "    \n",
    "    # Get the number of texts and the current index\n",
    "    num_texts = len(state['input_texts'])\n",
    "    current_index = state['current_index']\n",
    "    \n",
    "    # If the counter is less than the total number of texts, loop back\n",
    "    if current_index < num_texts:\n",
    "        return \"continue_loop\"\n",
    "    \n",
    "    # Otherwise, move to the accuracy analysis node\n",
    "    return \"analyze_accuracy\"\n",
    "\n",
    "# Build the LangGraph workflow\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add the nodes to the graph\n",
    "workflow.add_node(\"sentiment_analysis\", sentiment_analysis_node)\n",
    "workflow.add_node(\"accuracy_analysis\", accuracy_analysis_node)\n",
    "\n",
    "# Set the entry point and define the conditional edge\n",
    "workflow.set_entry_point(\"sentiment_analysis\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"sentiment_analysis\",\n",
    "    router,\n",
    "    {\n",
    "        \"continue_loop\": \"sentiment_analysis\",  # Loop back to the same node\n",
    "        \"analyze_accuracy\": \"accuracy_analysis\" # Move to the next node\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add the final edge to end the graph\n",
    "workflow.add_edge(\"accuracy_analysis\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef98ccf8",
   "metadata": {},
   "source": [
    "Experimenting with the agent to see whether it's working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b665d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing text 1: 'I love this new phone, it's so fast!'\n",
      "  -> Result: Positive\n",
      "Analyzing text 2: 'The customer service was terrible, I'm very disappointed.'\n",
      "  -> Result: Negative\n",
      "Analyzing text 3: 'This movie was okay, but not the best.'\n",
      "  -> Result: Negative\n",
      "Analyzing text 4: 'The new restaurant has amazing food and great prices.'\n",
      "  -> Result: Positive\n",
      "\n",
      "--- Starting Accuracy Analysis ---\n",
      "Final Accuracy Score: Let's analyze each text:\n",
      "\n",
      "1. Original Text: 'I love this new phone, it's so fast!' | Sentiment: Positive\n",
      "\t* Correct! The text expresses a strong positive sentiment.\n",
      "2. Original Text: 'The customer service was terrible, I'm very disappointed.' | Sentiment: Negative\n",
      "\t* Correct! The text expresses a strong negative sentiment.\n",
      "3. Original Text: 'This movie was okay, but not the best.' | Sentiment: Negative\n",
      "\t* Correct! Although the text doesn't use strong language, it implies that the movie is mediocre or below average, which is a negative sentiment.\n",
      "4. Original Text: 'The new restaurant has amazing food and great prices.' | Sentiment: Positive\n",
      "\t* Correct! The text expresses a strong positive sentiment about the restaurant.\n",
      "\n",
      "Overall Accuracy:\n",
      "Out of 4 texts, all 4 sentiments were correct. Therefore, the overall accuracy is:\n",
      "\n",
      "100%\n",
      "\n",
      "Note that I'll only output the percentage number (e.g., 75%).\n",
      "\n",
      "--- Final Output ---\n",
      "Original Texts: [\"I love this new phone, it's so fast!\", \"The customer service was terrible, I'm very disappointed.\", 'This movie was okay, but not the best.', 'The new restaurant has amazing food and great prices.']\n",
      "Sentiment Results: ['Positive', 'Negative', 'Negative', 'Positive']\n",
      "Calculated Accuracy Score: Let's analyze each text:\n",
      "\n",
      "1. Original Text: 'I love this new phone, it's so fast!' | Sentiment: Positive\n",
      "\t* Correct! The text expresses a strong positive sentiment.\n",
      "2. Original Text: 'The customer service was terrible, I'm very disappointed.' | Sentiment: Negative\n",
      "\t* Correct! The text expresses a strong negative sentiment.\n",
      "3. Original Text: 'This movie was okay, but not the best.' | Sentiment: Negative\n",
      "\t* Correct! Although the text doesn't use strong language, it implies that the movie is mediocre or below average, which is a negative sentiment.\n",
      "4. Original Text: 'The new restaurant has amazing food and great prices.' | Sentiment: Positive\n",
      "\t* Correct! The text expresses a strong positive sentiment about the restaurant.\n",
      "\n",
      "Overall Accuracy:\n",
      "Out of 4 texts, all 4 sentiments were correct. Therefore, the overall accuracy is:\n",
      "\n",
      "100%\n",
      "\n",
      "Note that I'll only output the percentage number (e.g., 75%).\n"
     ]
    }
   ],
   "source": [
    "# Example data for the agent to process\n",
    "initial_data = [\n",
    "    \"I love this new phone, it's so fast!\",\n",
    "    \"The customer service was terrible, I'm very disappointed.\",\n",
    "    \"This movie was okay, but not the best.\",\n",
    "    \"The new restaurant has amazing food and great prices.\"\n",
    "]\n",
    "\n",
    "# Run the graph with the initial state\n",
    "final_state = app.invoke({\n",
    "    \"input_texts\": initial_data,\n",
    "    \"current_index\": 0,\n",
    "    \"sentiment_results\": [],\n",
    "    \"accuracy_score\": \"\"\n",
    "})\n",
    "\n",
    "# Print the final results\n",
    "print(\"\\n--- Final Output ---\")\n",
    "print(f\"Original Texts: {final_state['input_texts']}\")\n",
    "print(f\"Sentiment Results: {final_state['sentiment_results']}\")\n",
    "print(f\"Calculated Accuracy Score: {final_state['accuracy_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b7a81b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68193b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63112513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb3cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc5b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b9d105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
